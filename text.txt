Okay.Good morning, good afternoon, good evening, everyone,depending on where in the world you are.Um, uh, welcome to our firstof our AI fireside chat series.Um, today, myself, uh, I'm Kevin McGee.I'm the CTO for Zeit.And, uh, Constantine, uh,Luan, who's the head of data science.You know, we're going to, you know,we're gonna have a chat today, um, about all things AIand what Zeit is doing in that space.We're gonna do this as a, as a, as a series, um,over the next, you know, uh, few weeks and months.So, uh, we're not gonna probably go into massive amountsof detail in every area, but in, in future sessions,you know, we'll, we'll do some live demos, you know,we'll bring some other people alongand have a, have a conversation.But today we're just gonna introduce some of the thingsthat we've been doing, you know, over the last few monthsand, you know, some of the upcoming things.And, you know, we, we'll go from there.So, with that in mind, um, Constantine,we've been doing AI in inside for, you know, many years now.You know, we've, you know, we've, we've, we've got,you know, uh, quite a bit of capability.Um, just briefly, you know, do you want to bring everybody,you know, up to speed on, you know, justwhat we've been doing and, you know, uh, and so on.And you're on mute. So we've started, well, yeah.Nice book. So, yeah. Hi. Hi, Kevin. Thanks for the intro.And yeah, I mean, I think we've been doing like at, at Zeitand previously Scraping Hub doing, um,like various projects for a while.I think the biggest one recently was the, uh,automatic extraction, which is now part of, uh, Zeit API,where the main goal is to, uh, do extractionof like a few different data points, uh, like data types,such as like article products,show postings from all kinds of pages.And the magic here is that it, um, like it works on, uh,any website, like it is trained on some websites,but we check how it works on websites it haven't seen.And this makes it like robust to like any website changes.And also it means that like we have people coming to usand they can hit, like run, run their website,like run their jobsand get the results, uh, immediately,which like makes the whole process much shorter.Yeah. I think putting this in context for anyonewho may not be familiar, like the significance of this, uh,I suppose is the traditional way of,of doing web scraping was to write a lot of codeand using, you know, frameworks like scrapeor other, other tools, you know,so software developers write a lot of code,and then of course, if the underlying website changed,you know, the, you know, you hadto rewrite the code and make adjustments.What we're talking aboutand what's been in place is this idea of, you know,essentially no code, you know, at scale, you know,that we've been bringing in, which is, which is incredible.Um, I, I want to pick up on somethingthat you were saying there as well that you, you, you kindof have been passing mentioned that, um, yes,we've had this auto extract product, um, in, in the marketfor, for quite a while,but we're, we're, we're sunsetting that individual project.We're rolling it into our, uh, design API product, you know,maybe just like, you know, bring, um, you know,the audience up to speed on why we're doing that.Uh, yeah. Yeah. So I think the main reason is what we, uh,see that, like, to extract the data we needto also download the page.And so automatic extraction handles bothdownloading and extraction.And, um, very often people wantto customize the download process.Like they want to maybe set a different location,maybe they perform some actions in the browseror like tweak some like yeah, JavaScript setting on and off.And, uh, this has a very big overlap with a general problemof downloading the pages,because some people still wantto do the extraction themselvesand they want to download, um, the data.And this is like what oneof the things which ITPI like the main thing,which API solves,but it didn't really make sense to have like one service,which does like downloading and extractionand another separate service, which does only downloading,and they have like different waysand like more limited waysof customizing the downloading part.So that's why we Yeah, put it, it makes a lot of sensebecause we can benefit from Yeah, better.Like API unified billing,the customer has only one bill to pay.And yeah, it just feels, I think, much better to use, um,like, and much we can have much more features with the same,uh, like size of the team.Yeah. Okay. Brilliant.Um, uh, also maybe to expand a little bit on, you know, the,the data types that we currently support in,in your traditional kind of extraction capability.Oh, yeah, yeah. So the, the main data,like the data types,which we see like the most used are product, which coversas like very large e-commerce, like large numberof e-commerce use cases, and then article.So article is normally, um, like it's, um, uh,like news websites,but it can also be, blogs can also be like any, any kindof like publications which look like an article.Uh, and then, um, we also have, for these two,we also have data types which helpto implement crawling more efficiently.Like, for example, uh, product navigation,which would get like the links to, uh, subcategories, linksto the next page in the list, links to digital products.And on top of that you can do the crawling.And then, um, yeah, same for, for, for article.And then we also have like, job posting, uh,and then we have like in, in automatic extraction, uh,we had a larger number of data types,but we only like carried over the ones which saw the mostadoption, uh, website, API,and then we'll be slowly moving them as like,as people see the need for them, uh, in inside API.Okay. No, brilliant.Um, I, I think one of the interesting, you know,and I kind of alluded to it, you know, a little bit earlier,um, you know, we're, we're getting really close to,you know, the, the requirement that for, for many,many use cases, you know, the need to write custom code is,is, is probably gonna be, you know, fewand far between is, is kind of our hope, you know,won't always be the case, but that's kind of our hope.But, you know, behind the scenes, you know,and we don't, we, we haven't released this, you know,publicly, but one of the things that we, we have createdand your team has, is, is kind of createdand built is, is this idea of, you know, kindof the vi visual annotationsand quick fixes that that really, um, speed up the abilityto retrain, you know, the, the models.And so it's a, a kind of a two-parter question one, well,why do we need to, to retrain the models?And, you know, and just a little bit about,you know, well, how does that work?Right? Right. Yeah.Like w we, we see that with machine learning instruction,as I say, yeah, it's like super nicethat it works on any website,but then, uh, as it's based on machine learning,it can make mistakes and it will make mistakes.And for some projects it can be like, not too bad,but for some projects we really want like,the best quality possible.And so, um, we like peopleor we would notice a, an error if we're doing a projectfor a client, or like a client would notice, um,like a incorrect extraction.And then like, uh, previously we would have to, okay,add this page and update it, retrain the model,and this whole cycle might take like weeks and,but now we can actually do this wholecycle in minutes instead.And the way we do this is that we can, uh, like, um,add some like hints to the model to guide itto the correct, uh, extraction.And, uh, this hints they're based, um,like based on like selecting specificlike areas on the page.And they, they don't have to be exact.For example, we would often see, okay, we are like,let's say we're picking up some price from related products,and instead of like having to specify exactly fromwhere we need to pick the price, we could just say, okay,don't look in this area of the page,and this area can be quite generic, just some containerwith related products.And that immediately excludes this area for the model,but then it is ableto pick up the correct price from the right,uh, part of the page.So this allows us to be quite robust to any, like,website changes or model changes.So that worst case we fall backto the default model behavior,but normally we, we just, um, uh, yeah, obtain correctand improved results this way.Yeah, I think it's, it's potentially one of those, uh,sessions that we might do down the line.It'd be great to show everyone just, uh, someof the capability that we have, you know, around doing that.And, you know, and I think it, it leads into this topicthat, you know, Zeit for many years has been driving this,um, this vision to, to remove the complexityand to remove the challenges that people have in this space.Um, and you know, you've, you know, we've,that's why the whole ideaof AI extraction was created in the first place.It's, you know, the whole reason why ZI API was created, it,it removes an awful lot of the challenges.But last year we created a, a, a capabilitythat was particularly revolutionary in this,in this space, you know?And so, you know, it's the, it's the abilityto do everything that you just described therefrom a raw HTML perspective.And, you know, I'd love for you to just expand maybe onthat, you know, one, well, why, why is this so,you know, revolutionary?Why is this so significant?And, you know, and, and, um, you know, and, and,and how is it differentor better than, you know, what has proceeded, um, and so on.Yeah, yeah, yeah.So when we started the project first, like, our idea wasthat like the quality is what is going to make or break it,because if we don't reach high enough quality, we, yeah,we're not going to be useful.And, uh, that's why we decided, okay, we want to like, putas much useful features as we can into the model.And that's why in additionto providing it like the HTML threeand like other, like the text of the page, we also took thescreenshot of the whole pageand, uh, found a clever way to combine this.Um, like the, the screenshot features with the page, uh,features and HTML level features.And, um, this is a bit similar tohow the human sees the, the page.Let's say you see, okay, this is like some big thing.It's like a title that's maybe a name of the productor the title of the article.And then you might see, okay, this is like clearly the priceand this small thing is not a price.Um, and this, this gave like a large boost in quality.But the drawback as we like, uh, carried over with this,we saw more and more, uh, like drawbackswith this, uh, approach.Like one is, uh, that the cost implication that, uh,browser rendering is always going to be more expensive than,uh, downloading just page thel of the page.And the difference can be like, yeah, five times or so.And another, another drawback is, um,sometimes a browser rendering would be, uh, less reliable,uh, because there might be like various reasonsfor the rendering to, to not work e exactly as we want.Um, although like on some websites, it is mandatoryto render it because we need to execute the JavaScript.So we, we still want this capability,but we thought, okay, can we do it without rendering?And initially we got like, quite a big drop in quality, uh,from that, uh,because the, the model didn't lacked this like,whole page understanding, whole page context.So we only looked at the individual parts of the page,but didn't understand how they relate to each other.And then we found out that actuallyby like adjusting the neural network architectureto like include some, like, uh, elements, uh,like recurrent neural networks there,we could actually like close the gap between the rawand rendered extraction.And then we, yeah, this is already released in ITPI,so you can, when doing the extraction, you can specifythat you want to use, uh, extraction from, uh, roach ml,and that would be like around 10,10 times faster on average.And also, uh, yeah, much cheaper as well.Yeah, no, it's, it's, you know,it's absolutely incredible, you know, toto, to get to that level.And, you know, for everybody listening to this,you'll start seeing, you know, us talk a bit more,uh, publicly about this.We've been running in beta programsand various things, you know, throughout, you know, the,the, the end of last year, you know,but, um, you know, we're, you know, we're, we're,we're going big with it, you know, now this, this year.So there'll be a a lot moreto come there, but it's already in the product.It's already there if anybody wants to tryand use it, you know, that's, that's fantastic.Okay. So just wanna move on to something slightly different,a little bit of a, you know, what's coming next?Where are we going, what are we trying to do?And if I channel this idea, you know, that we saidbefore, you know, which is driving everything that we doinside, we want to keep the costs down, we wantto remove the complexity, remove, you know, the challenges.One of the challenges that we would've got in the past ispeople talking about this, this idea, the AI is fantastic.You know, I can get my product, you know, schemaas you guys have specified it,but I've got a, you know, I'm in a particular industry.I'm, or I'm in a particular use case where there are,you know, maybe some extra things that are available,you know, that I, I now have to go and, you know,and I have to write some extra code,or I have to do something to get at the, at these extra bitsthat are not maybe in the, in the generic schemathat the model was originally trained for.And, you know, we, you know, you demoed, you know,for anybody who would've seen the, uh, the,the extract summit, um, you know, liveor the recordings, you know, you demoed our new capabilityaround custom attributes at that, at that event.You know, maybe you can just give us a little bitof background to, again, to that project,why we're doing it, you know, what we're, what, what we're,um, achieving with that now at this point. Yeah,Yeah, yeah. Yeah. That's,that's a very, very interesting development.So yeah, like we, we, we always noticethat in most projects, people always want something extra,uh, something custom, which doesn't make senseto include in the main model.Let's say it's, you are crawling like, uh, jigsaw puzzles,and you want to know what is the numberof pieces in each puzzle.And this field can be, like,spelled slightly differently on different websites.But, uh, yeah, if you, if you were to, uh, to get it,you'd need to write some codeand then going from zero code to some code, it,it immediately increases Yeah.The cost of the project and the time it takes.And yeah, so with like A-C-G-B-T excitement,and we saw immediately that, okay,this is something which, uh, it can do.And, um, like it can actually, uh, extract, um, uh,new attributes, uh, just by, uh,reading the user description of this attribute,basically the name, and sometimes, um, like some descriptionand the type of attribute.And also, um, like having the accessto the text of the webpage.Um, but also we saw that, like, we didn't want to use OpenAIAPIs because like, this is like problematicfor many customers to use like some third party APIsbecause they, they don't know okay, what will happen with,uh, with the data they're sending.And, uh, it's also like, we don't have any control.We can't guarantee low response timesand so on and so forth.So, and also there was like an explosion in open sourceecosystem with like people, uh, replicating, uh,these capabilities with like Facebookand meta releasing like Lama series of models.And so we saw that we can actually, uh, implement it using,um, like our old models,which are based on open source models,so we could fine tune them on this taskand basically allow, uh, you as an API user to just specify,okay, this is like extra attributes I want to include.And then, uh, we would fetch them, uh, using lms.And this is coming later this year.Yeah, no, it's, it's fantastic.I think we, I think that's probably gonna be one of our, uh,other talks that it's, it's worth doing,because I know your team, you know, did a, a, an evaluationof a lot of the, you know, the public models,the commercial ones, as well as, you know, manyof the open source, you know, ones and so on.I think it will be even just, just generally of interestto people, you know, what your team found and so on.So that's possibly one we can come back to.Um, I think it's, it, it's also, you know, really importantto, you know, to recognize that, um, our, you know, so, sothat's now taking the ability of, uh, you know,AI extraction at scale, then adding this ability to do,you know, custom attributesas we're just calling it also at scaleand at the, you know, price points that are, you know, um,you know, won't obviously be at the same cost as, you know,regular extraction because there's a bit more compute goingon, but, you know, but those price points will certainly be,you know, way, way lower than, um, you know, having tohand code it and do it all yourself.And the real beauty of all of this is the abilitythat it's effectively, you know, it's adaptive,it self maintains, you know, it reacts to what's going onwith the underlying capability, which is fantastic.Uh, another kind of, I suppose sneak peek to somethingthat's coming real soon is, you know, this ideathat we're calling, uh, AI scraping, you know,which builds on, you know, a lot of the, you know,the capability that we've talked about before,but comes at it from a slightly different angle.You know, maybe you could just explain, you know,what we have in mind there to everyone.Yeah, yeah, sure. Sowith zet PI like the main capability is that we can send itto URL and it would extract, let's say productor article, uh, or job postingor like any other data type attributes.Um, the problem with it, with this though, isthat normally people want to, uh,get the data like from one websiteor from like, even a lot of websites.And to do that, they need to like visit all the pages they,they want to, to get, uh, data from.And so this part would require some development.But then now we, uh, yeah, as I said,we also provide like models which can understand not onlyindividual product pages,but also like sections of the website, let's say a pagewith like a list of products or a list of categories.And they can do like extraction of links from them.And using this as primitives,we can actually write like a quite a generic, um, spider,which works on any websitebecause of this primitives work on any website.And so what it allows you to do is you can go to the ui,you can enter, let's say the website URLor like a part, like a section of the websiteand, um, like select the data typeand hit like, for now it's only products, uh, hit Go.And basically the, the crawler is running in scrappy cloudand it's getting, uh, getting you the data accordingto the product schema.Um, yeah. Andbecause like, there was like actually a similar feature in,uh, legacy out extract project,but uh, it was like a custom UI built for this.And now with that API, we optedto use the scrape cloud as a basis for this.And we also open sourcing, well,we already open sourced the crawler itself,so you can actually, um, if you want to tweak,let's say the crawling strategy, you can actually modify it,upload it to scrape cloud and run it,and yeah, to support all of that,like we added some special, uh, new featuresto Scrappy Cloud, which allow you to specify like,which arguments you want to basically satisfy the arguments,which provides a very nice user experience for that.And also, since it's scrapy cloud, it also has the api,so now you can control the crawls, uh,for the API, uh, as well.Yeah, no, it's, it's, it's a, it's, it's, uh,particularly exciting kinda bringing a lotof this together now, you know, where, you know, we,we have, you know, we're doing, you know, hundredsof millions of, you know, requests, you know, every month,you know, with, with, you know, around extraction requests,you know, we're, you know, we've got, you know, someof the largest organizations in the worldare using this capability.We use our, our whole whole delivery team are using,you know, many of these, you know, these capabilities.You know, and, and you know, we've, you know, uh, you know,as I said, we've, we, we we're dedicated as an organizationto, you know, to removing the complexity to kindof simplify all of this.We, we understand a lot of this is complicated,and so we're trying to, we're we're tryingto bring it together, but one last kind of, you know,teaser, you know, kind of piece related to, to, to AIthat I just want to touch on.Uh, and it's, it's back at this same point again about,you know, making things simple.So you mentioned earlier that, you know, nowwith extraction, we have, you have two choices as a,as a developer, you can, you can get the full rendered,you know, browser rendered version of, of, of extraction,or you can get the, the raw HT ML version of extraction.And you know, it, it, you know, obviously that's fantastic,it's great capability, but it means as a developer you haveto make a choice, you know, which should use,and maybe you have to do experiment,but we're working on somethingand maybe you could tell, you know,the audience a little bit about, you know,what we have in mind, you know,that'll be coming soon, or, or,Yeah. So yeah, thehere the problem isthat like the raw extraction is very attractivebecause it's, yeah, so cheap and fast,but it won't work on some websites if they requireJavaScript rendering.And like a lot of people, they need to crawl like hundredsof thousands of websites.And so like for them to check every website,whether which method to prefer, it's just not feasible.Uh, that's why we default, uh, to browser, uh,extraction right now, uh,because it's like, yeah, the most generic.So it, it will do a, like good job, uh, in all cases,but, uh, we may, yeah, missing out on these improvements.And so what we're working on is like this, uh, intelligent,uh, switching, uh, to raw extraction if we seethat it's actually providing, uh, good results.And, um, so, uh, later this year when you're making, uh,extraction without specifying exact method to use, uh, likebehind the scenes, we would, uh, switch to the best methodfor these websites.And this would happen, like completely transparently.And also there won't be like any delay here.So even from the first request, we would likebehind the scenes, uh, try, uh, both methods at onceand see which one is better,and then return you, uh, the best results.And also, uh, very quickly would switchto always using a particular downloading method,uh, for this website.Yeah, no, and, and of course our, our,our cunning master plan is to weave that inbehind the scenes behind AI scraping, behind, you know,general extraction behind the APIs.So, you know, we can, you know,just make it as easy as possible.But of course, in all cases, allow, you know,allow overrides, allow people to, you know,if they have a particular choice, you can do that as well.And, and of course, you know, we want to make sure that allof these things are available, you know,callable via our open source tools and, you know, and so on.So that's, that's, that's, uh, where we're coming from.So listen, uh, great chat, Constantine.You know, I think we, we covered a lot of, you know,what we're doing here, um, in the AI space.You know, we, we didn't even touch on some of the use casesthat, you know, our clients are doing.You know, we're, we're providing them with a whole lotof things that they're doing really exciting things from anAI perspective, but maybe we'll come back tothat in another, in another talk.Um, but, uh, thanks for your time today, Constantine,and you know, if, if,if anyone has any questions, you know where to find us.And so thanks all. Yeah,See you soon. Thank you. Yeah,thank you. See you.